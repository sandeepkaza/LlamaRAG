# ğŸ¦™ RAG System â€” Ollama Edition

A **fully local, production-ready RAG system** â€” no API keys, no cloud, no cost.  
Powered by **Ollama** for both LLM inference and embeddings, with a beautiful Streamlit UI.

![Python](https://img.shields.io/badge/Python-3.11+-blue?logo=python)
![Ollama](https://img.shields.io/badge/Ollama-local%20LLM-black?logo=llama)
![LangChain](https://img.shields.io/badge/LangChain-0.2+-green)
![Streamlit](https://img.shields.io/badge/Streamlit-1.35+-red?logo=streamlit)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## âœ¨ Features

| Feature | Details |
|---|---|
| **100% Local** | No API keys needed â€” runs entirely on your machine |
| **LLM Providers** | Ollama (default) Â· OpenAI Â· Anthropic |
| **Embedding Models** | Ollama Â· sentence-transformers Â· OpenAI |
| **Vector DBs** | ChromaDB (default) Â· FAISS Â· Pinecone Â· Qdrant |
| **Document Formats** | PDF Â· DOCX Â· TXT Â· Markdown Â· HTML Â· CSV Â· XLSX Â· URL |
| **Chunking Strategies** | Recursive Â· Sentence Â· Token |
| **Retrieval Strategies** | Similarity Â· MMR Â· Hybrid (BM25 + semantic) |
| **UI** | Streamlit app with streaming, sources, setup guide |
| **CLI** | `ingest`, `query`, `models`, `setup`, `ui`, `info` |
| **Tests** | Pytest suite |
| **Docker** | One-command deployment with Ollama bundled |

---

## ğŸ—ï¸ Architecture

```
User Question
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Streamlit UI / CLI                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG Chain                        â”‚
â”‚  1. Condense multi-turn question              â”‚
â”‚  2. Retrieve top-K chunks                     â”‚
â”‚  3. Format context + citations                â”‚
â”‚  4. Stream answer from Ollama                 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                           â”‚
       â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB   â”‚           â”‚  Ollama (local)   â”‚
â”‚  (vectors)  â”‚           â”‚  llama3.2 / etc.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²
       â”‚  ingestion
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load â†’ Chunk â†’ Embed (nomic-embed-text)    â”‚
â”‚  PDF / DOCX / TXT / MD / HTML / CSV / URL   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
rag-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ document_loader.py   # Multi-format loader
â”‚   â”‚   â”œâ”€â”€ chunker.py           # recursive|sentence|token
â”‚   â”‚   â”œâ”€â”€ embedder.py          # ollama|sentence-transformers|openai
â”‚   â”‚   â””â”€â”€ pipeline.py          # loadâ†’chunkâ†’embedâ†’store
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # chroma|faiss|pinecone|qdrant
â”‚   â”‚   â””â”€â”€ retriever.py         # similarity|mmr|hybrid
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ llm.py               # ollama|openai|anthropic
â”‚   â”‚   â”œâ”€â”€ prompts.py           # RAG + condense prompts
â”‚   â”‚   â””â”€â”€ rag_chain.py         # streaming conversational chain
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ app.py               # Streamlit app (4 tabs)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py              # pydantic-settings
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â””â”€â”€ test_config.py
â”œâ”€â”€ data/raw/                    # Drop documents here
â”œâ”€â”€ cli.py                       # Typer CLI
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- [Ollama](https://ollama.com/download) installed

### 1. Clone & Install

```bash
git clone https://github.com/sandeepkaza/LlamaRAG.git
cd LlamaRAG

python -m venv .venv

# Windows
.venv\Scripts\activate

# Mac/Linux
source .venv/bin/activate

pip install -r requirements.txt
```

### 2. Start Ollama & Pull Models

```bash
# Terminal 1 â€” keep this running
ollama serve

# Terminal 2 â€” pull the models (one time)
ollama pull llama3.2
ollama pull nomic-embed-text
```

### 3. Configure

```bash
cp .env.example .env
```

The defaults work out of the box for Ollama â€” no changes needed!

```dotenv
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434

EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=nomic-embed-text

VECTOR_DB=chroma
```

### 4. Run

```bash
streamlit run src/ui/app.py
```

Open http://localhost:8501 â†’ **Ingest** tab â†’ upload docs â†’ **Chat** tab â†’ ask questions! ğŸ‰

---

## ğŸ¦™ Recommended Ollama Models

### LLM Models

```bash
ollama pull llama3.2          # Best balance (default)
ollama pull llama3.2:1b       # Fastest, lowest RAM
ollama pull llama3.1          # Higher quality, more RAM
ollama pull mistral           # Great for European languages
ollama pull gemma2            # Google's model, very capable
ollama pull phi3              # Microsoft, very efficient
ollama pull qwen2.5           # Great for multilingual
ollama pull codellama         # Optimized for code
ollama pull deepseek-r1       # Strong reasoning
```

### Embedding Models

```bash
ollama pull nomic-embed-text     # Best quality (default)
ollama pull mxbai-embed-large    # High quality, larger
ollama pull all-minilm           # Fastest, smallest
```

### Good Combos by Use Case

| Use Case | LLM | Embedding |
|---|---|---|
| General Q&A | `llama3.2` | `nomic-embed-text` |
| Low RAM (<8GB) | `llama3.2:1b` | `all-minilm` |
| High quality | `llama3.1` | `nomic-embed-text` |
| Code assistant | `codellama` | `nomic-embed-text` |
| Reasoning | `deepseek-r1` | `nomic-embed-text` |

---

## ğŸ–¥ï¸ Streamlit UI

The app has 4 tabs:

**ğŸ’¬ Chat** â€” Multi-turn Q&A with streaming responses and source citations  
**ğŸ“¥ Ingest** â€” Upload files or paste a URL, with chunking controls  
**âš™ï¸ Setup** â€” Live setup checklist showing what's installed and running  
**ğŸ“Š Stats** â€” Ingestion history with charts  

Sidebar controls let you switch LLM model, embedding model, vector DB, retrieval strategy, and top-K in real time.

---

## âŒ¨ï¸ CLI Usage

```bash
# Check Ollama status
python cli.py setup

# List pulled models
python cli.py models

# Ingest a directory
python cli.py ingest --path data/raw/

# Ingest a single file
python cli.py ingest --file report.pdf --collection finance

# Ingest from URL
python cli.py ingest --url https://docs.example.com/page

# Query
python cli.py query "What is the main finding?"

# Query with specific model
python cli.py query "Summarize chapter 3" --strategy mmr --top-k 10 -m llama3.1

# Show current config
python cli.py info

# Launch UI
python cli.py ui
```

---

## ğŸ³ Docker

```bash
cp .env.example .env

# Start app + Ollama together
docker compose up -d

# Pull models inside the container
docker exec rag-ollama ollama pull llama3.2
docker exec rag-ollama ollama pull nomic-embed-text

# View logs
docker compose logs -f app
```

App at http://localhost:8501

---

## âš ï¸ Important: Switching Embedding Models

If you change `EMBEDDING_MODEL` or `EMBEDDING_PROVIDER`, you **must** delete the ChromaDB folder and re-ingest all documents. Different models produce vectors with different dimensions â€” mixing them causes errors.

```bash
# Windows
rmdir /s /q data\chroma_db

# Mac/Linux
rm -rf data/chroma_db
```

Or use the **Reset ChromaDB** button in the sidebar.

---

## ğŸ§ª Tests

```bash
pytest tests/ -v
```

---

## ğŸ’¡ Programmatic Usage

```python
from src.ingestion.pipeline import ingest_documents
from src.ingestion.embedder import get_embeddings
from src.retrieval.vector_store import get_vector_store
from src.generation.rag_chain import RAGChain

# Ingest
ingest_documents(["report.pdf"], collection_name="finance")

# Query
embeddings = get_embeddings()
store = get_vector_store(embeddings, collection_name="finance")
chain = RAGChain(store, top_k=5, strategy="mmr")

result = chain.invoke("What were the Q3 revenues?")
print(result["answer"])

# Streaming
for chunk in chain.stream("Summarize the report"):
    print(chunk, end="", flush=True)
```

---

## ğŸ”§ Troubleshooting

**`Connection refused` / Ollama not running**
```bash
ollama serve
```

**Model not found**
```bash
ollama pull llama3.2
ollama pull nomic-embed-text
```

**ChromaDB dimension mismatch** â€” Delete `data/chroma_db/` and re-ingest after changing embedding model.

**Slow first response** â€” Ollama loads the model into memory on first use. Subsequent queries are much faster.

**Out of memory** â€” Switch to a smaller model: `ollama pull llama3.2:1b`

---

## ğŸ“„ License

MIT

---

## ğŸ¤ Contributing

```bash
pytest tests/ -v
black src/ cli.py
isort src/ cli.py
```
