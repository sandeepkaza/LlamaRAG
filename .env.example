# ============================================================
# RAG System - Environment Variables
# Copy this file to .env and fill in your values
# ============================================================

# ── Ollama (default — fully local, no API key needed) ────────
OLLAMA_BASE_URL=http://localhost:11434

# ── LLM Provider ─────────────────────────────────────────────
# Options: ollama | openai | anthropic
LLM_PROVIDER=ollama

# Ollama models (run: ollama pull <model>)
LLM_MODEL=llama3.2
# Other good options: llama3.1, mistral, gemma2, phi3, qwen2.5

# OpenAI (only if LLM_PROVIDER=openai)
# OPENAI_API_KEY=sk-proj-...
# LLM_MODEL=gpt-4o-mini

# Anthropic (only if LLM_PROVIDER=anthropic)
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-3-5-haiku-20241022

# ── Embedding Model ───────────────────────────────────────────
# Options: ollama | sentence-transformers | openai
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=nomic-embed-text
# Other Ollama embed options: mxbai-embed-large, all-minilm

# sentence-transformers (local, no Ollama needed)
# EMBEDDING_PROVIDER=sentence-transformers
# EMBEDDING_MODEL=all-MiniLM-L6-v2

# ── Vector Database ───────────────────────────────────────────
# Options: chroma | faiss | pinecone | qdrant
VECTOR_DB=chroma
CHROMA_PERSIST_DIR=./data/chroma_db

# FAISS (in-memory, good for testing)
# VECTOR_DB=faiss

# Pinecone (cloud)
# VECTOR_DB=pinecone
# PINECONE_API_KEY=pcsk-...
# PINECONE_INDEX_NAME=rag-index
# PINECONE_ENVIRONMENT=us-east-1

# Qdrant (self-hosted)
# VECTOR_DB=qdrant
# QDRANT_URL=http://localhost:6333

# ── Chunking ──────────────────────────────────────────────────
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNK_STRATEGY=recursive

# ── Retrieval ─────────────────────────────────────────────────
RETRIEVAL_TOP_K=5
RETRIEVAL_STRATEGY=similarity

# ── App ───────────────────────────────────────────────────────
APP_TITLE=RAG System
LOG_LEVEL=INFO
